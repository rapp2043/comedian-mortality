{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMm2PRvrA80XJQr1gYk78fz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rapp2043/comedian-mortality/blob/main/comedian_mortality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Web scraper program for list of comedians on Wikipedia\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "def get_comedians_list(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    comedians = []\n",
        "\n",
        "    # Find all the <a> tags within the <li> tags\n",
        "    for li_tag in soup.find_all('li'):\n",
        "        a_tag = li_tag.find('a')\n",
        "        if a_tag and 'href' in a_tag.attrs:\n",
        "            link = a_tag.attrs['href']\n",
        "            if link.startswith('/wiki/'):\n",
        "                # Construct the full URL\n",
        "                full_url = \"https://en.wikipedia.org\" + link\n",
        "                comedians.append({'name': a_tag.text, 'url': full_url})\n",
        "\n",
        "    return comedians\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = ['name', 'url']\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        writer.writeheader()\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "# URL of the Wikipedia page\n",
        "url = 'https://en.wikipedia.org/wiki/List_of_comedians'\n",
        "\n",
        "# Get the list of comedians and their URLs\n",
        "comedians_list = get_comedians_list(url)\n",
        "\n",
        "# Save the data to a CSV file\n",
        "save_to_csv(comedians_list, 'comedians.csv')\n",
        "\n",
        "print(\"CSV file has been created with the list of comedians.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GVYBHxcWo-h",
        "outputId": "659a2ff0-9a01-448f-99db-73820ff688d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file has been created with the list of comedians.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Function to extract bday and dday from a Wikipedia URL\n",
        "def extract_dates(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        bday_tag = soup.find('span', class_='bday')\n",
        "        dday_tag = soup.find('span', class_='dday')\n",
        "\n",
        "        bday = bday_tag.get_text() if bday_tag else None\n",
        "        dday = dday_tag.get_text() if dday_tag else None\n",
        "\n",
        "        return bday, dday\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Requests error for URL {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Input and output CSV filenames\n",
        "input_csv = '/content/comedians.csv'\n",
        "output_csv = 'comedian_birth_death_dates2.csv'\n",
        "\n",
        "# Reading the input CSV and writing the extracted information to the output CSV\n",
        "with open(input_csv, mode='r', newline='', encoding='utf-8') as infile, \\\n",
        "     open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
        "\n",
        "    reader = csv.DictReader(infile)\n",
        "    writer = csv.DictWriter(outfile, fieldnames=['name', 'url', 'birth_date', 'died_date'])\n",
        "    writer.writeheader()\n",
        "\n",
        "    for row in reader:\n",
        "        print(f\"Processing: {row['name']}\")\n",
        "        birth_date, died_date = extract_dates(row['url'])\n",
        "        writer.writerow({'name': row['name'], 'url': row['url'], 'birth_date': birth_date, 'died_date': died_date})"
      ],
      "metadata": {
        "id": "JFDejrnvsTmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting Birth and Death Dates from Individual Wikipedia Pages\n",
        "\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define a function to extract birth and death dates from a Wikipedia page\n",
        "def extract_dates(wiki_url):\n",
        "    try:\n",
        "        # Send a request to the Wikipedia page\n",
        "        response = requests.get(wiki_url)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        # Parse the response content with BeautifulSoup\n",
        "        soup = BeautifulSoup(response.content, 'lxml')\n",
        "\n",
        "        # Try to extract the birth date from the 'bday' class\n",
        "        bday_tag = soup.find('span', class_='bday')\n",
        "        birth_date = bday_tag.get_text() if bday_tag else None\n",
        "\n",
        "        # Try to extract the death date from the 'dday' class or by looking for 'Died' text\n",
        "        death_date = None\n",
        "        dday_tag = soup.find('span', class_='dday')\n",
        "        if dday_tag:\n",
        "            death_date = dday_tag.get_text()\n",
        "        else:\n",
        "            # Search for the 'Died' table header and try to extract the following text\n",
        "            died_cell = soup.find('th', string=lambda text: text and 'Died' in text)\n",
        "            if died_cell:\n",
        "                death_date = died_cell.find_next('td').get_text().strip()\n",
        "\n",
        "        return birth_date, death_date\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error fetching {wiki_url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Read the input CSV using pandas\n",
        "input_csv = '/content/comedians.csv'\n",
        "output_csv = 'comedian_birth_death_dates2.csv'\n",
        "\n",
        "comedian_list = pd.read_csv(input_csv)\n",
        "\n",
        "# Prepare columns for the birth and death dates\n",
        "comedian_list['birth_date'] = None\n",
        "comedian_list['death_date'] = None\n",
        "\n",
        "# Iterate over the dataframe rows\n",
        "for index, row in comedian_list.iterrows():\n",
        "    print(f\"Processing {row['name']}...\")\n",
        "    birth_date, death_date = extract_dates(row['url'])\n",
        "    comedian_list.at[index, 'birth_date'] = birth_date\n",
        "    comedian_list.at[index, 'death_date'] = death_date\n",
        "\n",
        "# Write the updated dataframe to a new CSV file\n",
        "comedian_list.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"Finished processing. Birth and death dates can be found in '{output_csv}'\")"
      ],
      "metadata": {
        "id": "7PIpM8QFyxwQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}